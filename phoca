#!/usr/bin/python3
import argparse, sys, os, time, concurrent.futures, random, csv, io
import pickle
from sklearn.ensemble import RandomForestClassifier
from tqdm import tqdm
from pandas import DataFrame, read_csv, concat

sys.path.insert(1, os.path.join(os.path.dirname(__file__), 'probes'))

from timingProbes import *
from featureProbes import *

class Detector:

    timingProbes = [
        tcpSYNTiming,
        tlsClientHelloTiming,
        tlsClientHelloErrorTiming,
        tlsHandshakeTiming,
        httpsGetRequestTiming,
        httpGetRequestTiming,
        httpGetRequestErrorTiming,
        httpsGetRequestErrorTiming,
        httpGetRequestNoHostHeaderTiming,
        httpsGetRequestNoHostHeaderTiming
    ]

    featureProbes = [
        TLSLibrary,
        TLSVersions,
    ]

    def __init__(self, http_port=80, https_port=443, numIterations=10,
            chunkSize=-1, modelFile="./classifierDrop.cls"):

        self.http_port = http_port
        self.https_port = https_port
        self.numIterations = numIterations
        self.chunkSize = chunkSize
        self.modelFile = modelFile

        # Create a tmp file for caching output so memory usage doesn't balloon
        self.tmpFile = '.PhocaCache'
        if(self.chunkSize != -1):
            f = open(self.tmpFile, 'w')
            f.close()

    def clean_up(self):
        if(os.path.isfile(self.tmpFile)):
            os.remove(self.tmpFile)

    def crawl(self, domains, outputFile=None, classify=False, rawData=False):
        crawlResults = {}

        if(classify):
            self.model = pickle.load(open(self.modelFile, 'rb'))

        # If there are more than 1 URL, we want to display a progress bar on the screen
        with concurrent.futures.ThreadPoolExecutor(10) as executor:
            for result in self.tqdm_parallel_map(executor, self.testSite, [(domain, classify) for domain in domains]):
                crawlResults[result['site']] = {'classification' : result['classification'], 'data' : result['data']}

                if(self.chunkSize != -1 and len(crawlResults) > self.chunkSize):
                    self.writeResultsToFile(self.readChunkedResults(crawlResults), self.tmpFile, file_format='json', classify=classify, rawData=rawData)
                    crawlResults = []

        results = self.readChunkedResults(crawlResults)
        output = self.writeResultsToFile(results, outputFile, classify=classify, rawData=rawData)
        if(outputFile == None):
            pass

        self.clean_up()

        return results

    def testSite(self, site, classify):
        result = {'site' : site}
        result['data'] = self.probeSite(site)
        result['classification'] = None

        if(classify):
            result['classification'] = self.classifySite(result['data'])

        return result

    def classifySite(self, recordings):
        # result = {'site' : recordings['site'], 'classification' : None}
        classification = None

        recordingsDataFrame = DataFrame([recordings])
        columnsToDrop = [column for column in recordingsDataFrame if column not in self.model.feature_names]
        recordingsDataFrame = recordingsDataFrame.drop(columnsToDrop, axis=1)
        if(recordingsDataFrame.isna().sum().sum() > 0):
            return classification

        recordingsDataFrame = recordingsDataFrame.reindex(sorted(recordingsDataFrame.columns), axis=1)

        try:
            classification = self.model.predict(recordingsDataFrame)[0]
            print(classification)
        except Exception as e:
            print(e)

        return classification

    def probeSite(self, site):
        probeResults = {'site' : site}

        # Place all feature probes into threadpool queue
        executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
        featureProbeThreads = []
        for probe in Detector.featureProbes:
            featureProbeThreads.append(executor.submit(probe(site, self.http_port, self.https_port).test))

        # On the main thread, loop through the timing threads so the main thread does something
        # while the feature threads are running
        for probe in Detector.timingProbes:
            currentProbeResults = probe(site, self.http_port, self.https_port).test(n=self.numIterations)
            probeResults[probe.__name__] = currentProbeResults

        # Compute the additional timing features
        probeResults['httpsGetSynRatio'] = probeResults['httpsGetRequestTiming'] / probeResults['tcpSYNTiming']
        probeResults['httpGetSynRatio'] = probeResults['httpGetRequestTiming'] / probeResults['tcpSYNTiming']
        probeResults['httpsGetErrorSynRatio'] = probeResults['httpsGetRequestErrorTiming'] / probeResults['tcpSYNTiming']
        probeResults['httpGetErrorSynRatio'] = probeResults['httpGetRequestErrorTiming'] / probeResults['tcpSYNTiming']
        probeResults['httpGetHttpGetErrorRatio'] = probeResults['httpGetRequestTiming'] / probeResults['httpGetRequestErrorTiming']
        probeResults['httpsGetHttpsGetErrorRatio'] = probeResults['httpsGetRequestTiming'] / probeResults['httpsGetRequestErrorTiming']

        # Collect the results of the feature threads
        for thread in featureProbeThreads:
            try:
                probeResults.update(thread.result(timeout=60))
            except Exception as e:
                raise e
                probeResults.update({})

        executor.shutdown(wait=False)

        return probeResults

    def readChunkedResults(self, results):
        existingFileDataFrame = {}
        if(os.path.exists(self.tmpFile) and os.path.getsize(self.tmpFile) > 0):
            with open(self.tmpFile, 'r') as f:
                existingFileDataFrame = json.load(f)
        results.update(existingFileDataFrame)
        return results

    def writeResultsToFile(self, siteResults, outputFile=None, file_format='csv', classify=False, rawData=False):
        if(outputFile != None):
            f = open(outputFile, 'w')
        else:
            f = io.StringIO()

        if(file_format == 'csv'):
            resultsToFile = []

            for key,value in siteResults.items():
                currentResults = {}
                if(rawData):
                    currentResults.update(value['data'])
                if(classify):
                    currentResults['classification'] = value['classification']
                currentResults['site'] = key

                resultsToFile.append(currentResults)

            writer = csv.DictWriter(f, fieldnames=resultsToFile[0].keys())
            writer.writeheader()
            for row in resultsToFile:
                writer.writerow(row)
        elif(file_format == 'json'):
            for key in siteResults.keys():
                if(not classify):
                    del siteResults[key]['classification']
                elif(not rawData):
                    del siteResults[key]['data']

            with open(outputFile, 'w') as f:
                json.dump(siteResults, f)

        if(outputFile == None):
            output = f.getvalue()
        else:
            output = None
        f.close()
        return output

    def tqdm_parallel_map(self, executor, fn, *iterables, **kwargs):
        """
        Equivalent to executor.map(fn, *iterables),
        but displays a tqdm-based progress bar.

        Does not support timeout or chunksize as executor.submit is used internally

        **kwargs is passed to tqdm.
        """
        futures_list = []
        results = []
        for iterable in iterables:
            futures_list += [executor.submit(fn, i[0], i[1]) for i in iterable]
        if(len(futures_list) > 1):
            for f in tqdm(concurrent.futures.as_completed(futures_list), total=len(futures_list), **kwargs):
                yield f.result()
        else:
            for f in concurrent.futures.as_completed(futures_list):
                yield f.result()

def process_args():
    programDescription = """
    ######################################
     _____  _    _  ____   _____
    |  __ \| |  | |/ __ \ / ____|   /\\
    | |__) | |__| | |  | | |       /  \\
    |  ___/|  __  | |  | | |      / /\ \\
    | |    | |  | | |__| | |____ / ____ \\
    |_|    |_|  |_|\____/ \_____/_/    \_\\
    
    ######################################
    """

    parser = argparse.ArgumentParser(description=programDescription, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("domain",
                        nargs="?",
                        help="Domain to classify as a MITM phishing website. Not required if input file specified with -r argument.")
    parser.add_argument("-R", "--record",
                        action="store_true",
                        default=False,
                        help="Set operating mode to record data about site(s). Output argument required if this mode is set.")
    parser.add_argument("-w", "--output-file",
                        type=str,
                        help="File to write probe outputs to. This argument is required if in record mode.")
    parser.add_argument("-r", "--input-file",
                        type=str,
                        help="File containing URLs or IP addresses to crawl. Each line should contain only the URL.")
    parser.add_argument("-n", "--num-iterations", type=int, default=10,
                        help="Number of times each timing probe should be executed for each site. A larger number of " + \
                                "iterations per site will result in more accurate results, but a longer runtime. " +\
                                "This value defaults to 10.")
    parser.add_argument("--http-port", type=int, default=80,
                        help="Set the port to scan HTTP web servers. Defaults to 80.")
    parser.add_argument("--https-port", type=int, default=443,
                        help="Set the port to scan HTTPS web servers. Defaults to 443.")
    parser.add_argument("-c", "--chunk-size", type=int, default=-1,
                        help="Enable and set the size of chunking to use. By default all output will be stored in memory " + \
                                "unless a chunk size is specified here. A value of -1 means no chunking will be used. Must be >= -1.")
    args = vars(parser.parse_args())

    if(args["domain"] == None and args["input_file"] == None):
        parser.print_help(sys.stderr)
        sys.exit(1)
    elif(os.geteuid() != 0):
        print("Root permissions not granted. Removing TCP SYN/ACK timing from probe list. Rerun program as root to enable this probe.")
        self.timingProbes.remove(tcpSYNTiming)
    elif(args["chunk_size"] < -1):
        parser.print_help(sys.stderr)
        sys.exit(1)
    return args

if(__name__ == '__main__'):
    args = process_args()

    if(args['input_file'] != None):
        with open(args['input_file'], "r") as f:
            domains = [domain.strip() for domain in f.readlines()]
            random.shuffle(domains)
    else:
        domains = [args["domain"]]

    detector = Detector(http_port=args['http_port'], https_port=args['https_port'], numIterations=args['num_iterations'],
            chunkSize=args['chunk_size'])

    detector.crawl(domains, outputFile=args['output_file'], classify=not args['record'], rawData=args['record'])
